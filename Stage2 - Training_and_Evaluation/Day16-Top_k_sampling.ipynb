{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61209336",
   "metadata": {},
   "source": [
    "#### **Top-K Sampling**\n",
    "\n",
    "Top-k sampling restricts token selection to only the **top K most likely tokens**, preventing extremely low-probability tokens from being sampled.\n",
    "\n",
    "In temperature sampling alone, even very low-probability tokens still have a small chance of being selected.  \n",
    "Top-K fixes this by **keeping only the top K logits** and masking the rest.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**\n",
    "\n",
    "**Logits:**  \n",
    "`[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]`\n",
    "\n",
    "**Top-k = 3**\n",
    "\n",
    "Step 1 — **Select top 3 highest logits**  \n",
    "Top 3 logits are:  \n",
    "- 6.75  \n",
    "- 6.28  \n",
    "- 4.51  \n",
    "\n",
    "Step 2 — **Mask all other logits with `-inf`**  \n",
    "This removes them from the softmax calculation.\n",
    "\n",
    "**Masked logits:**  \n",
    "`[4.51, -inf, -inf, 6.75, -inf, -inf, -inf, 6.28, -inf]`\n",
    "\n",
    "Step 3 — **Apply temperature scaling (optional)**  \n",
    "\n",
    "Step 4 — **Apply softmax**  \n",
    "\n",
    "**Softmax probabilities:**  \n",
    "`[0.06, 0, 0, 0.57, 0, 0, 0, 0.36, 0]`\n",
    "\n",
    "Step 5 — **Sample from these probabilities**  \n",
    "Sampling occurs **only among the top-K tokens**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insight**\n",
    "\n",
    "Top-K sampling ensures that:\n",
    "\n",
    "- Only the top **K** most likely tokens have a chance to be selected  \n",
    "- Extremely unlikely (noisy) tokens are completely removed  \n",
    "- The model remains creative, but controlled  \n",
    "\n",
    "**→ The next token will *always* be chosen from the top K logits.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f80461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model \n",
    "import torch\n",
    "from transformer_blocks import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "gpt_model = GPTModel(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3551406b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the weights\n",
    "state_dict = torch.load(\"checkpoints/gpt_model.pth\", map_location='cpu')\n",
    "gpt_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b0a0ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.rand(2, 3, 7)\n",
    "t2 = torch.topk(t1, k=3, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4395cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ee17a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "# function that applies temperature scaling and top_k sampling\n",
    "def generate_next_token(model, top_k, temp, inputs):\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs)\n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k != None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_value = top_logits[:, -1].unsqueeze(1)\n",
    "            logits = torch.where(logits < min_value, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "        # now use we'll use temeprature scaling\n",
    "        if temp != 0.0:\n",
    "            logits = logits / temp \n",
    "            # now do the softmax\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            # do the sampling from probability distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1) # (batch_size, 1)\n",
    "        else:\n",
    "            next_token = torch.argmax(logits, dim=-1, keepdim=True) # (batch_size, 1)\n",
    "\n",
    "        return next_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aec1bacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randint(0, 50257, (2, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f90ce59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[314],\n",
       "        [339]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_next_token(gpt_model, 3, 0.3, inputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5061d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
