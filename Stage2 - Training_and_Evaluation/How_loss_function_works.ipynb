{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0223d5ba",
   "metadata": {},
   "source": [
    "## üîç How Loss Is Calculated While Training Transformers (Step-by-Step)\n",
    "\n",
    "Training a transformer boils down to **comparing predicted token probabilities with the correct target tokens** at every position in the sequence.  \n",
    "Below is a complete walkthrough of how this works mathematically and in practice.\n",
    "\n",
    "---\n",
    "\n",
    "### ### üìå Example Input\n",
    "\n",
    "**Input Tokens (B = 2, T = 4):**\n",
    "\n",
    "[[128, 576, 320, 547],\n",
    "[657, 943, 633, 547]]\n",
    "\n",
    "**Target Tokens (next token for each position):**\n",
    "\n",
    "[[576, 320, 547, 897],\n",
    "[943, 633, 547, 210]]\n",
    "\n",
    "Here:\n",
    "- **B = 2** ‚Üí batch size  \n",
    "- **T = 4** ‚Üí sequence length  \n",
    "- **V = 50,257** ‚Üí vocabulary size for GPT-2  \n",
    "\n",
    "---\n",
    "\n",
    "#### üß† Step 1: Forward Pass ‚Üí Logits\n",
    "\n",
    "When the input goes through the model, the transformer outputs **logits**, not probabilities.\n",
    "\n",
    "**Logits shape:** \n",
    "[Batch_size, Time_steps, Vocabulary_size] = (B, T, V)\n",
    "\n",
    "Each of the 4 positions in each sequence gets a probability distribution over the entire vocab.\n",
    "\n",
    "---\n",
    "\n",
    "#### üßÆ Step 2: Convert Logits ‚Üí Probabilities (Softmax)\n",
    "\n",
    "Softmax is applied along the vocabulary dimension:<br>\n",
    "probs = softmax(logits, dim=-1)<br>\n",
    "This calculates the probabilities of each word fighting for that position\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÅ Step 3: Reshape (Flatten) Logits and target ids\n",
    "Logits shape : (B, T, V)\n",
    "We convert it to : (B*T, V)\n",
    "\n",
    "new_logits = logits.flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "Traget ids shape : (B, T)\n",
    "New target shape : (B*T)\n",
    "\n",
    "#### Now we have to look for propabilities corresponding to these target ids at each context / time-step\n",
    "\n",
    "-> probabilites of shape (B*T, 1)\n",
    "\n",
    "-> find average of these probs \n",
    "\n",
    "-> take negative log and this is the loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "347eec82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are input tokens to LLM : torch.Size([2, 4])\n",
      "Input tokens : tensor([[11486, 31563,  6140, 17682],\n",
      "        [13134, 22911, 20243, 43382]])\n",
      "Taregt tokens : tensor([[31563,  6140, 17682, 18369],\n",
      "        [22911, 20243, 43382, 45413]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "input_tokens = torch.randint(0, 50257, (2, 4))\n",
    "target_tokens = torch.zeros_like(input_tokens)\n",
    "target_tokens[:, :-1] = input_tokens[:, 1:]\n",
    "target_tokens[:, -1] = torch.randint(0, 50257, (2,))\n",
    "print(f\"These are input tokens to LLM : {input_tokens.shape}\")\n",
    "print(f'Input tokens : {input_tokens}')\n",
    "print(f'Taregt tokens : {target_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873cd8f9",
   "metadata": {},
   "source": [
    "Now we have input_tokens and target_tokens, we'll send the input tokens to LLM which give output of shape [2, 4, 50257]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6bdb0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of logits returned by LLM : torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "# after passing input ids from LLM it returns logits of shape (B, T, V)\n",
    "logits = torch.rand((2, 4, 50257))\n",
    "print(f\"Shape of logits returned by LLM : {logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74eae1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after applying softmax : torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "# we'll take softmax of the last dim as it represent all words for that position, softmax will give probabilities of each word \n",
    "import torch.nn.functional as F\n",
    "logits_softmax = F.softmax(logits, dim=-1)\n",
    "print(f\"Shape after applying softmax : {logits_softmax.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcc150af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[17322],\n",
      "         [34935],\n",
      "         [32629],\n",
      "         [18804]],\n",
      "\n",
      "        [[ 2298],\n",
      "         [40166],\n",
      "         [ 7312],\n",
      "         [27669]]])\n"
     ]
    }
   ],
   "source": [
    "# Now for each position what will be the predicted token\n",
    "# Token with hoghest probability will be the predicted token for that position\n",
    "predicted_token_ids = torch.argmax(logits_softmax, dim=-1, keepdim=True)\n",
    "print(predicted_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0800eeeb",
   "metadata": {},
   "source": [
    "See for batch 1 and position 0 - the token with highest probability is with id 17322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cd35422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target tokens shape : torch.Size([2, 4])\n",
      "Logits shape : torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "# but for calculating loss we need target ids\n",
    "# so we'll take probabilities corresponding to target ids \n",
    "# Negative Log likelihood - take log -> calculate mean -> multiply -1 (This will give loss)\n",
    "\n",
    "print(f\"Target tokens shape : {target_tokens.shape}\")\n",
    "print(f\"Logits shape : {logits_softmax.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5bebc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of targets flatten : torch.Size([8])\n",
      "Shape of logits flatten : torch.Size([8, 50257])\n"
     ]
    }
   ],
   "source": [
    "targets_flatten = target_tokens.flatten(start_dim=0)\n",
    "logits_flatten = logits_softmax.flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "print(f\"Shape of targets flatten : {targets_flatten.shape}\")\n",
    "print(f\"Shape of logits flatten : {logits_flatten.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c774cd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1652e-05],\n",
       "        [2.0611e-05],\n",
       "        [2.5886e-05],\n",
       "        [2.8628e-05],\n",
       "        [1.3507e-05],\n",
       "        [1.5790e-05],\n",
       "        [2.0037e-05],\n",
       "        [2.1681e-05]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_probabilities = logits_flatten.gather(1, targets_flatten.unsqueeze(1))\n",
    "target_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fb60fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take negative log \n",
    "log_probs = -1*torch.log(target_probabilities)\n",
    "log_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21a0fc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 10.874761581420898\n"
     ]
    }
   ],
   "source": [
    "# take mean of negative logs\n",
    "log_probs_mean = log_probs.sum()/(log_probs.shape[0])\n",
    "\n",
    "print(f\"Loss : {log_probs_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55227620",
   "metadata": {},
   "source": [
    "#### **Using torch.nn.functional.cross_entropy()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1985446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of target tokens : torch.Size([2, 4])\n",
      "Shape of predicted output : torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "# now one line of code will do all the things we did above calculating softmax then loss\n",
    "# we have target_tokens and predicted_output\n",
    "print(f\"Shape of target tokens : {target_tokens.shape}\")\n",
    "print(f\"Shape of predicted output : {logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8835e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 10.874760627746582\n"
     ]
    }
   ],
   "source": [
    "logits_flatten = logits.flatten(start_dim=0, end_dim=1)\n",
    "targets_flatten = target_tokens.flatten(start_dim=0)\n",
    "\n",
    "loss = torch.nn.functional.cross_entropy(logits_flatten, targets_flatten)\n",
    "\n",
    "print(f\"Loss : {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b9031",
   "metadata": {},
   "source": [
    "### **Perplexity Score**\n",
    "\n",
    "Perplexity is a widely used metric to evaluate language models.  \n",
    "It provides a more interpretable measure of how *uncertain* the model is when predicting the next token.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Definition**\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} = e^{\\text{loss}}\n",
    "$$\n",
    "\n",
    "- Lower perplexity ‚Üí **model is more confident and better at prediction**  \n",
    "- Higher perplexity ‚Üí **model is unsure and closer to guessing randomly**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Example**\n",
    "\n",
    "Suppose the model‚Äôs cross-entropy loss is: 10.87\n",
    "\n",
    "\n",
    "Then perplexity is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} = e^{10.87} \\approx 52575\n",
    "$$\n",
    "\n",
    "This value means:\n",
    "\n",
    "- The model is as uncertain as if it had to choose the next token **uniformly at random**\n",
    "  from **~52,575 possible vocabulary items**.\n",
    "- High perplexity = poor prediction quality.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Interpretation**\n",
    "\n",
    "- **Perplexity ‚Üì** ‚Üí Model is learning better token distributions  \n",
    "- **Perplexity ‚Üë** ‚Üí Model is confused or the dataset/model capacity is insufficient  \n",
    "- Ideal models (like trained GPTs) usually have perplexity in low double-digits on test sets.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c594c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
