{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d500c638",
   "metadata": {},
   "source": [
    "## üî• **Temperature Scaling in Language Models**\n",
    "\n",
    "Temperature is a hyperparameter that controls how *deterministic* or *random* the next-token predictions are during sampling.\n",
    "\n",
    "After the model outputs **logits**, we normally apply **softmax** to convert them into probabilities.  \n",
    "With temperature scaling, we modify logits before softmax:\n",
    "\n",
    "$$\n",
    "p_i = \\text{softmax}\\left(\\frac{\\text{logits}_i}{T}\\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ### üßä **Low Temperature (T < 1) ‚Äî More Deterministic**\n",
    "Example: **T = 0.1**\n",
    "\n",
    "- Large logits become *even larger*, small logits become *even smaller*  \n",
    "- Probability distribution becomes **sharper**\n",
    "- Model picks the most likely token almost every time\n",
    "- Useful for:\n",
    "  - Technical writing  \n",
    "  - Math / reasoning  \n",
    "  - Code generation  \n",
    "  - Factual responses  \n",
    "  - When you need consistent, reliable outputs  \n",
    "\n",
    "---\n",
    "\n",
    "### üî• **High Temperature (T > 1) ‚Äî More Creative / Random**\n",
    "Example: **T = 5**\n",
    "\n",
    "- Logits become **more similar**  \n",
    "- Even low-probability tokens get a chance  \n",
    "- Probability distribution becomes **flatter**\n",
    "- Output becomes more diverse and creative\n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê **Temperature = 1 ‚Üí Normal Sampling**\n",
    "Temperature of **1** does nothing:\n",
    "\n",
    "$$\n",
    "\\frac{\\text{logits}}{1} = \\text{logits}\n",
    "$$\n",
    "\n",
    "Softmax produces normal probabilities.\n",
    "\n",
    "The model samples tokens **exactly according to their true probability distribution**.\n",
    "\n",
    "---\n",
    "\n",
    "## üé≤ **How Sampling Works (Multinomial Sampling)**\n",
    "\n",
    "After softmax, we have probabilities.  \n",
    "We sample using:\n",
    "\n",
    "```python\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4df1901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let say we have this small vocabulary \n",
    "\n",
    "word2idx = {\n",
    "    'I' : 0,\n",
    "    'am' : 1, \n",
    "    'learning' : 2,\n",
    "    'continuously' : 3,\n",
    "    'for' : 4\n",
    "}\n",
    "\n",
    "idx2word = {i : word for word, i in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20112e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8823, 0.9150, 0.3829, 0.9593, 0.3904])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets get a random vector of size of vocab \n",
    "import torch \n",
    "torch.manual_seed(42)\n",
    "logits = torch.rand(5)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b55da6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2309, 0.2385, 0.1401, 0.2493, 0.1412])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply softmax to convert them to probs\n",
    "probs = logits.softmax(dim=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47838c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning': 157, 'am': 223, 'I': 222, 'continuously': 249, 'for': 149}\n"
     ]
    }
   ],
   "source": [
    "# from above token at index 3 is most likely (high probability)\n",
    "# To generate text with more variety, we can replace the argmax with a function that samples from a probability distribution (here, the probability scores the LLM generates for each vocabulary entry at each token generation step).\n",
    "sampling_result = {}\n",
    "for _ in range(1000):\n",
    "    sampled_id = torch.multinomial(probs, num_samples=1).item()\n",
    "    sampled_word = idx2word[sampled_id]\n",
    "\n",
    "    # update dictionary counts\n",
    "    if sampled_word not in sampling_result:\n",
    "        sampling_result[sampled_word] = 0\n",
    "    \n",
    "    sampling_result[sampled_word] += 1\n",
    "\n",
    "print(sampling_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7147a71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.4567e-04, 1.1767e-02, 9.1224e-26, 9.8779e-01, 1.9476e-25])\n",
      "{'continuously': 987, 'am': 13}\n"
     ]
    }
   ],
   "source": [
    "## now lets try with lowering the temperature \n",
    "temp = 0.01\n",
    "logits2 = logits/temp \n",
    "probs2 = logits2.softmax(dim=-1)\n",
    "print(probs2)\n",
    "\n",
    "## this increase probability of word with highest logit and reduce down others (more deterministic)\n",
    "\n",
    "sampling_result2 = {}\n",
    "for _ in range(1000):\n",
    "    sampled_id = torch.multinomial(probs2, num_samples=1).item()\n",
    "    sampled_word = idx2word[sampled_id]\n",
    "\n",
    "    # update dictionary counts\n",
    "    if sampled_word not in sampling_result2:\n",
    "        sampling_result2[sampled_word] = 0\n",
    "    \n",
    "    sampling_result2[sampled_word] += 1\n",
    "\n",
    "print(sampling_result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec7ca3a",
   "metadata": {},
   "source": [
    "See only word at index 4 is sampled most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d50cdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2058, 0.2069, 0.1893, 0.2084, 0.1896])\n",
      "{'I': 210, 'for': 196, 'learning': 166, 'continuously': 206, 'am': 222}\n"
     ]
    }
   ],
   "source": [
    "## now lets try with increase the temperature \n",
    "temp = 6\n",
    "logits3 = logits/temp \n",
    "probs3 = logits3.softmax(dim=-1)\n",
    "print(probs3)\n",
    "\n",
    "## this increase probability of word with highest logit and reduce down others (more deterministic)\n",
    "\n",
    "sampling_result3 = {}\n",
    "for _ in range(1000):\n",
    "    sampled_id = torch.multinomial(probs3, num_samples=1).item()\n",
    "    sampled_word = idx2word[sampled_id]\n",
    "\n",
    "    # update dictionary counts\n",
    "    if sampled_word not in sampling_result3:\n",
    "        sampling_result3[sampled_word] = 0\n",
    "    \n",
    "    sampling_result3[sampled_word] += 1\n",
    "\n",
    "print(sampling_result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af26a1a",
   "metadata": {},
   "source": [
    "All word gets some chance to appear"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
