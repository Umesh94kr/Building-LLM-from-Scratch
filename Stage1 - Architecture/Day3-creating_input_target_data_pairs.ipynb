{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b0824a",
   "metadata": {},
   "source": [
    "In this section, we‚Äôll implement a function that generates **input‚Äìtarget pairs** for training a language model.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Why We Don‚Äôt Need Labeled Data\n",
    "\n",
    "Language model training is **self-supervised** ‚Äî  \n",
    "we don‚Äôt need human-labeled datasets like in classification or regression.\n",
    "\n",
    "Instead, we can use the **text itself** to create labels:\n",
    "\n",
    "> Each token predicts the **next token** in the sequence.\n",
    "\n",
    "For example:\n",
    "\n",
    "Then the model learns pairs like:\n",
    "| Input | Target |\n",
    "|--------|--------|\n",
    "| I | love |\n",
    "| I love | deep |\n",
    "| I love deep | learning |\n",
    "\n",
    "---\n",
    "\n",
    "### üåÄ The Twist: Sliding Window\n",
    "\n",
    "We‚Äôll use a **sliding window** approach to efficiently create these input‚Äìtarget pairs.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Steps\n",
    "\n",
    "1. **Define a Context Window**\n",
    "   - The context window represents the **maximum number of tokens** (words, subwords, or characters) the model can see at once.  \n",
    "   - Example: `context_window = 5`\n",
    "\n",
    "2. **Slide Through the Text**\n",
    "   - Start from the beginning of the text and take **chunks of size = context_window**.\n",
    "   - For each position in the text:\n",
    "     - The **input** is a sequence of up to `context_window` tokens.\n",
    "     - The **target** is the **next token** following that sequence.\n",
    "\n",
    "3. **Repeat Until End of Text**\n",
    "   - Keep shifting the window by 1 token and continue generating pairs until you reach the end.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c501f9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data \n",
    "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0302d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens : 5145\n",
      "Sample tokens : [40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438]\n"
     ]
    }
   ],
   "source": [
    "# create token_ids\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "data_tokens = tokenizer.encode(raw_data)\n",
    "print(f\"Total tokens : {len(data_tokens)}\")\n",
    "print(f\"Sample tokens : {data_tokens[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0692d8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : [40, 367, 2885, 1464]\n",
      "y :     [367, 2885, 1464, 1807]\n"
     ]
    }
   ],
   "source": [
    "# context size\n",
    "context_size = 4\n",
    "enc_sample = data_tokens[:100]\n",
    "# if input is 4 tokens [1, 2, 3, 4], then output should be [2, 3, 4, 5]\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x : {x}\")\n",
    "print(f\"y :     {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4245f7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs : [40] ---> 367\n",
      "inputs : [40, 367] ---> 2885\n",
      "inputs : [40, 367, 2885] ---> 1464\n",
      "inputs : [40, 367, 2885, 1464] ---> 1807\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    inputs = enc_sample[:i]\n",
    "    output = enc_sample[i]\n",
    "\n",
    "    print(f\"inputs : {inputs} ---> {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8779bcfb",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader\n",
    "\n",
    "A Dataset is a Python class that tells PyTorch:<br>\n",
    "-> How to access your data<br>\n",
    "-> How many samples you have<br>\n",
    "-> How to fetch any item by index<br>\n",
    "\n",
    "DataLoader takes a Dataset and handles:\n",
    "\n",
    "‚úîÔ∏è Batching (e.g., batch size 32)<br>\n",
    "‚úîÔ∏è Shuffling (important for training)<br>\n",
    "‚úîÔ∏è Parallel data loading using workers<br>\n",
    "‚úîÔ∏è Dropping leftover samples<br>\n",
    "‚úîÔ∏è Putting tensors on GPU automatically (with pin_memory)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "029c44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing efficient Dataloaders that iterate over data and return batches of  PyTorch tensors\n",
    "import torch\n",
    "from torch.utils.data import Dataset \n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, raw_text, tokenizer, context_size, stride):\n",
    "        self.input_ids = []\n",
    "        self.output_ids = []\n",
    "        token_ids = tokenizer.encode(raw_text)\n",
    "\n",
    "        for i in range(0, len(token_ids) - context_size - 1, stride):\n",
    "            x = token_ids[i:i+context_size]\n",
    "            y = token_ids[i+1:context_size+i+1]\n",
    "            self.input_ids.append(torch.tensor(x))\n",
    "            self.output_ids.append(torch.tensor(y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.output_ids[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "188623bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_dataset = GPTDatasetV1(raw_data, tokenizer, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edf26e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5140\n"
     ]
    }
   ],
   "source": [
    "print(len(gpt_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "631a3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : tensor([  40,  367, 2885, 1464])\n",
      "y :  tensor([ 367, 2885, 1464, 1807])\n"
     ]
    }
   ],
   "source": [
    "# printing an example \n",
    "x = gpt_dataset[0][0]\n",
    "y = gpt_dataset[0][1]\n",
    "\n",
    "print(f\"x : {x}\")\n",
    "print(f\"y :  {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a0f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a DataLoader \n",
    "\n",
    "def create_dataloader_v1(raw_data, batch_size=4, context_size=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    dataset = GPTDatasetV1(raw_data, tokenizer=tokenizer, context_size=context_size, stride=stride)\n",
    "\n",
    "    # batch_size : number of batches model process before UPDATING parameters\n",
    "    # num_workers : parallel processing\n",
    "\n",
    "    # If your dataset size is not divisible by the batch_size, you‚Äôll end up with one last smaller batch.\n",
    "    # The drop_last flag controls whether to keep or drop that final partial batch.\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94558e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 314,  550, 3750,  ..., 6451,   11,  286],\n",
       "         [6164,   25,  366,  ...,   11, 4844,  286],\n",
       "         [ 286, 1762,   30,  ...,  388,  351,  262],\n",
       "         [2612, 4369,   11,  ...,  655, 4030,  465]]),\n",
       " tensor([[  550,  3750,   351,  ...,    11,   286,  2612],\n",
       "         [   25,   366, 16773,  ...,  4844,   286,   262],\n",
       "         [ 1762,    30,  2011,  ...,   351,   262,  1459],\n",
       "         [ 4369,    11,   523,  ...,  4030,   465,  2951]])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_data)\n",
    "batch = next(iter(dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22b1a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
