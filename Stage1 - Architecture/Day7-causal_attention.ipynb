{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbed5d3e",
   "metadata": {},
   "source": [
    "### â›” Causal (Masked) Attention\n",
    "\n",
    "When building **contextualized embeddings**, standard self-attention allows *every token* to attend to *every other token* â€” including **future tokens**.\n",
    "\n",
    "But wait... ðŸ‘€  \n",
    "Doesn't that create a **bias**?  \n",
    "Yes! Because when predicting the next word, the model **should not peek** at words that come *after* it.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ’­ Example\n",
    "\n",
    "Consider the sentence:\n",
    "\n",
    "> \"Hey how are you?\"\n",
    "\n",
    "If we allow full attention, then:\n",
    "\n",
    "- Embedding of **\"Hey\"** = depends on *\"Hey\", \"how\", \"are\", \"you\"*  \n",
    "- Embedding of **\"How\"** = depends on *\"Hey\", \"how\", \"are\", \"you\"*  \n",
    "- Embedding of **\"are\"** = depends on *\"Hey\", \"how\", \"are\", \"you\"*  \n",
    "- Embedding of **\"you\"** = depends on *\"Hey\", \"how\", \"are\", \"you\"*\n",
    "\n",
    "That means even the **first word** already â€œknowsâ€ all the **future words**, which is *unrealistic* for language generation.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ§  Correct Behavior (Causal Masking)\n",
    "\n",
    "To fix this, we use **causal attention** (also known as **masked self-attention**).  \n",
    "It ensures that each token can only attend to **previous** and **current** tokens â€” not future ones.\n",
    "\n",
    "$$\n",
    "\\text{Allowed attention: only to past and current positions.}\n",
    "$$\n",
    "\n",
    "So now:\n",
    "\n",
    "| Token | Can Attend To | Masked Tokens |\n",
    "|:------|:---------------|:--------------|\n",
    "| **\"Hey\"** | \"Hey\" | \"how\", \"are\", \"you\" |\n",
    "| **\"How\"** | \"Hey\", \"How\" | \"are\", \"you\" |\n",
    "| **\"are\"** | \"Hey\", \"How\", \"are\" | \"you\" |\n",
    "| **\"you\"** | \"Hey\", \"How\", \"are\", \"you\" | â€” |\n",
    "\n",
    "---\n",
    "\n",
    "#### âš™ï¸ How It Works\n",
    "\n",
    "Before applying **softmax** on the attention scores,  \n",
    "we apply a **mask** on all *future positions* (upper triangular part of the matrix):\n",
    "\n",
    "$$\n",
    "\\text{mask}[i, j] =\n",
    "\\begin{cases}\n",
    "0 & \\text{if } j \\leq i \\\\\n",
    "-\\infty & \\text{if } j > i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "$$\n",
    "\\text{Attention} = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} + \\text{mask} \\right) V\n",
    "$$\n",
    "\n",
    "This forces all future tokens (where \\( j > i \\)) to have **zero attention probability** after softmax.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸš€ In Summary\n",
    "\n",
    "- **Problem:** Standard attention lets words see the future (data leakage).  \n",
    "- **Solution:** Causal masking blocks future tokens.  \n",
    "- **Effect:** The model becomes **autoregressive** â€” it learns to predict the *next token* using only *past* context.  \n",
    "- **Used in:** GPT, GPT-2, GPT-3, GPT-4, and all autoregressive LLMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f44b4d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3714, 0.4833, 0.1388, 0.0561, 0.0588],\n",
       "        [0.2616, 0.0533, 0.2163, 0.4509, 0.6847],\n",
       "        [0.2808, 0.2020, 0.4746, 0.1538, 0.7563],\n",
       "        [0.5564, 0.0253, 0.7019, 0.7511, 0.6994]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets take a input sentence\n",
    "import torch\n",
    "context_len = 4\n",
    "embed_dim = 5\n",
    "\n",
    "inputs = torch.rand(4, 5)\n",
    "# 4 words each with embedding dim of 5 \n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac8af23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will define query, key, value weight matrices\n",
    "Wq = torch.nn.Parameter(torch.rand(embed_dim, embed_dim))\n",
    "Wk = torch.nn.Parameter(torch.rand(embed_dim, embed_dim))\n",
    "Wv = torch.nn.Parameter(torch.rand(embed_dim, embed_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60f35af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of queries : torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "# query, key and value embeddings\n",
    "queries = inputs @ Wq \n",
    "keys = inputs @ Wk \n",
    "values = inputs @ Wv \n",
    "\n",
    "print(f\"Shape of queries : {queries.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df247241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8263, 2.2394, 2.7503, 3.8324],\n",
       "        [2.2149, 2.8085, 3.4401, 4.7019],\n",
       "        [2.8050, 3.5435, 4.3073, 5.9037],\n",
       "        [3.9791, 5.0077, 6.0843, 8.3803]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we'll find attention scores \n",
    "attn_scores = queries @ keys.T # should be a 4x4 matrix for our use case\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6ccd0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we want will make a mask that when applied to attn_scores then upper half become -infinity, so that when we take softmax those becomes 0\n",
    "\n",
    "mask = torch.triu(torch.ones(context_len, context_len), diagonal=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9730743d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8167,   -inf,   -inf,   -inf],\n",
       "        [0.9906, 1.2560,   -inf,   -inf],\n",
       "        [1.2544, 1.5847, 1.9263,   -inf],\n",
       "        [1.7795, 2.2395, 2.7210, 3.7478]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll multiply it by -infinity \n",
    "attn_scores_masked = attn_scores.masked_fill(mask.bool(), -torch.inf) / torch.sqrt(torch.tensor(embed_dim, dtype=torch.float32))\n",
    "attn_scores_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "121f0c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4340, 0.5660, 0.0000, 0.0000],\n",
       "        [0.2299, 0.3199, 0.4502, 0.0000],\n",
       "        [0.0813, 0.1287, 0.2083, 0.5817]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply softmax \n",
    "attn_scores_scaled = torch.softmax(attn_scores_masked, dim=-1)\n",
    "attn_scores_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a6eb611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0175, 0.4273, 0.4822, 0.4923, 0.4856],\n",
       "        [1.2470, 0.8506, 0.5102, 0.7445, 0.8242],\n",
       "        [1.4388, 1.0627, 0.5218, 0.8687, 0.9431],\n",
       "        [1.9589, 1.5466, 0.8183, 1.3098, 1.4762]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use these attention weight to calculate contextualized words\n",
    "contextualixed_inputs = attn_scores_scaled @ values\n",
    "contextualixed_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9d66fe",
   "metadata": {},
   "source": [
    "#### Causal Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f2ed2491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, dim, context_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.Wq = torch.nn.Parameter(torch.rand(dim, dim))\n",
    "        self.Wk = torch.nn.Parameter(torch.rand(dim, dim))\n",
    "        self.Wv = torch.nn.Parameter(torch.rand(dim, dim))\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.register_buffer( 'mask',torch.triu(torch.ones(context_len, context_len), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is input senetence (batch_size, context_len, embedding_dim)\n",
    "        # calculate key, query and value\n",
    "        batch_size, context_len, emb_dim = x.shape\n",
    "        keys = x @ self.Wk\n",
    "        queries = x @ self.Wq \n",
    "        values = x @ self.Wv \n",
    "\n",
    "        # attention scores \n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        # scaling attention scores \n",
    "        # mask = torch.triu(torch.ones(x.shape[0], x.shape[0]), diagonal=1)\n",
    "        attn_scores = attn_scores.masked_fill_(self.mask.bool()[:context_len, :context_len], -torch.inf)\n",
    "\n",
    "        attn_scores = attn_scores / torch.sqrt(torch.tensor(x.shape[1], dtype=torch.float32))\n",
    "\n",
    "        scaled_attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        scaled_attn_weights = self.dropout(scaled_attn_weights)\n",
    "        contextualized_inputs = scaled_attn_weights @ values \n",
    "        return contextualized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ba81c8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4855, 0.6808, 0.0109, 0.0573],\n",
       "        [0.9897, 0.2111, 0.7847, 0.7631],\n",
       "        [0.0547, 0.4976, 0.6631, 0.6122],\n",
       "        [0.9288, 0.4945, 0.2420, 0.9873],\n",
       "        [0.1955, 0.6456, 0.5109, 0.1696],\n",
       "        [0.7426, 0.5118, 0.6644, 0.7572]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attention = CausalAttention(4, 6)\n",
    "inputs2 = torch.rand(6, 4)\n",
    "inputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c2c6303d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 4])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now we'll process the bacth of inputs (2 sentences)\n",
    "batch = torch.stack((inputs2, inputs2), dim=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bdb0ea42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 4])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after applying causal attention the shape should be same \n",
    "batch_output = causal_attention(batch)\n",
    "batch_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc5e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
