{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55cd51ed",
   "metadata": {},
   "source": [
    "### ✅ Shortcut Connections in Transformers\n",
    "\n",
    "Deep neural networks often suffer from the **vanishing gradient problem**, where the gradient of the loss with respect to model parameters becomes extremely small.  \n",
    "This causes the weight updates to become negligible, making learning very slow or stagnant.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ How Skip Connections Help\n",
    "\n",
    "Transformers use **residual (skip) connections** of the form:\n",
    "\n",
    "$$\n",
    "x_2 = x_1 + \\text{layer}_1(x_1)\n",
    "$$\n",
    "\n",
    "This simple addition dramatically improves gradient flow.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Gradient Flow Through a Skip Connection\n",
    "\n",
    "We want the gradient of the loss with respect to the input \\( x_1 \\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_1}\n",
    "  = \\frac{\\partial L}{\\partial x_2} \\cdot \\frac{\\partial x_2}{\\partial x_1}\n",
    "$$\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "x_2 = x_1 + \\text{layer}_1(x_1)\n",
    "$$\n",
    "\n",
    "Differentiate:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_2}{\\partial x_1}\n",
    "= 1 + \\frac{\\partial\\, \\text{layer}_1(x_1)}{\\partial x_1}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_1}\n",
    "= \\frac{\\partial L}{\\partial x_2} \\cdot \n",
    "\\left( 1 + \\frac{\\partial\\, \\text{layer}_1(x_1)}{\\partial x_1} \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Why This Helps\n",
    "\n",
    "- Even if  \n",
    "  $$\n",
    "  \\frac{\\partial\\, \\text{layer}_1(x_1)}{\\partial x_1}\n",
    "  \\approx 0\n",
    "  $$\n",
    "  (vanishing gradient inside the layer)\n",
    "\n",
    "- The **“1” term remains**, preserving gradient flow.\n",
    "\n",
    "This means the gradient **cannot vanish completely** — it always has at least the identity path.\n",
    "\n",
    "This is why residual connections are essential in **Transformers, ResNets, LLMs**, and other deep architectures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b1cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class GeLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d256bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets make a deep neural network\n",
    "\n",
    "class DeepNeuralNet(nn.Module):\n",
    "    def __init__(self, use_skip_connnection, layer_size):\n",
    "        super().__init__()\n",
    "        self.use_skip_connection = use_skip_connnection\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_size[0], layer_size[1], GeLU())),\n",
    "            nn.Sequential(nn.Linear(layer_size[1], layer_size[2], GeLU())),\n",
    "            nn.Sequential(nn.Linear(layer_size[2], layer_size[3], GeLU())),\n",
    "            nn.Sequential(nn.Linear(layer_size[3], layer_size[4], GeLU())),\n",
    "            nn.Sequential(nn.Linear(layer_size[4], layer_size[5], GeLU()))\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_skip_connection and layer_output.shape == x.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9afa8c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [25, 25, 43, 43, 54, 54]\n",
    "\n",
    "input1 = torch.rand(2, 3, 25)\n",
    "neural_net = DeepNeuralNet(True, layer_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46bbe409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 54])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = neural_net(input1)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e967904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
