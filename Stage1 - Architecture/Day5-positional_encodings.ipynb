{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e18ed1e",
   "metadata": {},
   "source": [
    "### üß≠ Positional Encoding  \n",
    "\n",
    "Word embeddings allow us to represent words in **numerical format**, but they don‚Äôt capture **the order or position** of words in a sentence.  \n",
    "To help models understand *where* each word appears, we use **Positional Encoding**.\n",
    "\n",
    "---\n",
    "\n",
    "#### üß© Why Do We Need It?\n",
    "Transformers don‚Äôt process tokens sequentially (like RNNs do), so they need an additional way to encode **token position information**.  \n",
    "Positional encodings are **vectors added to word embeddings** ‚Äî both having the same dimensionality ‚Äî so the model can infer *sequence order*.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öôÔ∏è Types of Positional Encoding\n",
    "\n",
    "1. **Sinusoidal (Fixed) Encoding ‚Äî used in the original Transformer paper**\n",
    "   - Position information is represented using sine and cosine functions of different frequencies.\n",
    "   - This allows the model to generalize to longer sequences even beyond what it was trained on.\n",
    "\n",
    "   $$\n",
    "   PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
    "   $$\n",
    "\n",
    "   where:  \n",
    "   - \\( pos \\) = position of the word  \n",
    "   - \\( i \\) = dimension index  \n",
    "   - \\( d_{model} \\) = embedding dimension\n",
    "\n",
    "---\n",
    "\n",
    "2. **Learned Positional Embeddings ‚Äî used in GPT models**\n",
    "   - Instead of using sine/cosine formulas, the model **learns position embeddings** as trainable parameters, just like word embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìè Example (GPT-2)\n",
    "| Parameter | Value |\n",
    "|------------|--------|\n",
    "| **Vocabulary Size** | 50,527 tokens |\n",
    "| **Embedding Dimension** | 768 |\n",
    "\n",
    "---\n",
    "\n",
    "#### üí° Intuition\n",
    "By combining **word embeddings** (what the word means) with **positional encodings** (where the word appears),  \n",
    "Transformers can understand both **content** and **order** ‚Äî which is essential for generating coherent text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb090337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data \n",
    "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "408409a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "print(raw_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "581b95b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Dataset and Dataloader class to make input - target pairs and also making batches\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "class CreateGPTDataV1(Dataset):\n",
    "    def __init__(self, raw_data, tokenizer, context_size, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(raw_data)\n",
    "\n",
    "        for i in range(0, len(token_ids) - context_size - 1, stride):\n",
    "            x = token_ids[i:i+context_size] # input \n",
    "            y = token_ids[i+1:context_size+i+1] # expected output\n",
    "            self.input_ids.append(torch.tensor(x))\n",
    "            self.target_ids.append(torch.tensor(y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "452f4c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a dataloader function\n",
    "import tiktoken\n",
    "def create_dataloader(raw_data, batch_size=8, context_size=4, stride=2, shuffle=True, drop_last=True, num_workers=0):\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    dataset = CreateGPTDataV1(raw_data, tokenizer=tokenizer, context_size=context_size, stride=stride)\n",
    "\n",
    "    # batch_size : number of batches model process before UPDATING parameters\n",
    "    # num_workers : parallel processing\n",
    "\n",
    "    # If your dataset size is not divisible by the batch_size, you‚Äôll end up with one last smaller batch.\n",
    "    # The drop_last flag controls whether to keep or drop that final partial batch.\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b70a87db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4eceb6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids of random batch : tensor([[   11,  3181,   503,   287],\n",
      "        [   13,   314,   531, 10722],\n",
      "        [ 7543,   284,   607,   599],\n",
      "        [  262,  6678, 40315, 10455],\n",
      "        [ 9091,    11,   393,  5017],\n",
      "        [  502,  6609,  1474,   683],\n",
      "        [  257,  1310,  4295,   438],\n",
      "        [   11,   326, 14516,   530]])\n",
      "Target ids of random batch : tensor([[ 3181,   503,   287,   262],\n",
      "        [  314,   531, 10722,   292],\n",
      "        [  284,   607,   599,  6321],\n",
      "        [ 6678, 40315, 10455,   546],\n",
      "        [   11,   393,  5017,   510],\n",
      "        [ 6609,  1474,   683,   318],\n",
      "        [ 1310,  4295,   438,    40],\n",
      "        [  326, 14516,   530,   286]])\n"
     ]
    }
   ],
   "source": [
    "input1, target1 = next(iter(dataloader))\n",
    "print(f\"Input ids of random batch : {input1}\")\n",
    "print(f\"Target ids of random batch : {target1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81511cc4",
   "metadata": {},
   "source": [
    "#### Createing Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9c8ce47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding of a particular index is : tensor([-1.2059, -1.4148,  0.1592,  0.7264, -0.6956,  0.5645, -0.1026,  0.8552,\n",
      "        -0.5472, -0.5667, -0.0869, -0.8797, -0.1199,  0.2968,  0.2860,  0.5360,\n",
      "         1.2170,  0.1915,  1.4244, -1.7213,  0.0454,  0.0283, -0.5049, -1.8557,\n",
      "         1.2699, -1.0043, -0.9414, -0.3488,  1.0302,  1.3187,  0.1226,  0.0232,\n",
      "        -0.7217, -0.9127,  0.2719, -0.6029, -0.4522,  0.0854,  2.1252,  0.8918,\n",
      "         0.8632, -1.1154,  1.1197,  0.2170,  0.1302,  0.3208, -0.9809,  0.2683,\n",
      "        -1.0661, -2.1820,  0.1911,  1.1133, -0.9528, -0.5476,  0.1193,  0.9452,\n",
      "        -1.6690,  0.6063,  1.2096,  0.8946,  0.5395, -1.0538,  1.0637,  0.8040,\n",
      "        -0.0799,  0.9757, -0.1454,  0.4125, -0.2916,  1.3593,  1.2002,  0.8287,\n",
      "        -1.0826, -1.2300, -0.8765,  1.4432, -1.4710, -1.9401,  0.2504, -1.1035,\n",
      "         1.9566,  0.0928, -0.3151, -0.2481,  2.0366, -0.8721, -0.1955, -1.9092,\n",
      "        -0.1083, -0.2639,  1.0048, -0.7462, -1.0388,  0.4919,  0.0065,  0.6546,\n",
      "         2.6045, -0.6136,  0.7359,  2.2864,  1.1266, -0.5953,  1.7847, -0.5849,\n",
      "        -2.0904,  0.2703, -0.1251, -1.5663,  0.5324, -1.0544,  0.9253, -0.2741,\n",
      "        -0.3406,  1.0145, -0.2393, -2.1402, -0.0466, -0.9280, -0.6626,  0.0456,\n",
      "         0.2371,  0.1424,  0.7464, -0.4505,  0.1328,  1.4784, -0.3518,  0.8661,\n",
      "        -0.2233, -0.7711,  0.5619,  0.8065,  0.8312, -0.7392,  0.0108, -1.9043,\n",
      "        -0.1469, -0.8911,  1.1361, -0.3051,  0.3596, -0.3617, -0.1070,  0.3489,\n",
      "         0.6929, -1.1311, -0.4457,  0.6342,  0.8792,  1.8413, -0.4926,  0.3225,\n",
      "        -1.3063,  1.3181, -0.0747,  1.1049, -1.9331,  0.0842,  0.3126, -1.0267,\n",
      "         0.4418, -0.0441,  0.3663, -1.2259,  0.0502, -0.5109, -1.9870, -0.9793,\n",
      "        -0.1102,  1.1749,  0.0437, -0.8741, -0.0259,  0.8109,  0.2534,  0.5942,\n",
      "         1.2891,  0.4411, -1.3521, -0.4233,  0.2327, -0.2215,  0.1470, -0.8255,\n",
      "        -0.8758,  1.1660, -1.8028,  0.3849, -1.2583,  0.0901,  0.7733,  0.5855,\n",
      "        -0.1869,  0.2062, -0.1801, -0.1480, -1.5837,  0.7366, -0.1946, -2.2597,\n",
      "         1.6857, -0.5489,  0.6220,  0.0866,  0.2461, -0.0185, -0.9516, -1.4374,\n",
      "        -0.5031, -0.1235, -0.2143,  0.1226,  1.6964,  0.5080, -0.0240,  1.8264,\n",
      "        -0.5691,  0.5313, -0.3279,  0.2005,  0.2688,  0.2701,  0.0999, -1.0159,\n",
      "        -0.4291,  0.7703, -0.4038, -0.5884, -0.1340,  1.8286,  0.0653,  0.5234,\n",
      "         0.4197, -0.3499, -1.0532, -0.7039,  0.5471,  1.1898,  1.6483, -0.5791,\n",
      "        -0.0663, -0.1207, -0.6081, -0.1868, -1.4867,  0.2469,  0.1622, -0.1348,\n",
      "         0.2527, -0.9551,  0.4782,  0.1150,  0.8486,  0.8618, -0.8662,  0.1609],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50527\n",
    "n_dim = 256\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, n_dim)\n",
    "print(f\"Embedding of a particular index is : {embedding_layer(torch.tensor(5))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3ef52d",
   "metadata": {},
   "source": [
    "#### Creating Positional Encodings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "143d71c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1b32c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of positional encodings : torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 # this represents the position 1, 2, 3, and 4\n",
    "n_dim = 256 \n",
    "\n",
    "positional_encoding_layer = torch.nn.Embedding(context_size, n_dim)\n",
    "# torch.arange(4) will give a tensor of tensor([0, 1, 2, 3])\n",
    "positional_encodings = positional_encoding_layer(torch.arange(context_size))\n",
    "print(f\"Shape of positional encodings : {positional_encodings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0222caa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of batch of token_ids : torch.Size([8, 4])\n",
      "Shape of batch of embeddings : torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Now making embedding of first input batch \n",
    "print(f\"Shape of batch of token_ids : {input1.shape}\")\n",
    "embds_input1 = embedding_layer(input1)\n",
    "print(f\"Shape of batch of embeddings : {embds_input1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7c7c782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after adding positonal encodings : torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Now we need to add those positional encodings \n",
    "embds_after_position = embds_input1 + positional_encodings\n",
    "print(f\"Shape after adding positonal encodings : {embds_after_position.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2184dcac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
