{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e4b6b8",
   "metadata": {},
   "source": [
    "### üöÄ Normalization in Neural Networks\n",
    "\n",
    "Normalization helps keep the values inside a neural network at a **stable scale**, ensuring that training is:\n",
    "\n",
    "- fast  \n",
    "- stable  \n",
    "- resistant to exploding/vanishing gradients  \n",
    "- smooth and predictable  \n",
    "\n",
    "Without normalization, networks can:\n",
    "\n",
    "- **explode** (values grow too large)  \n",
    "- **vanish** (values shrink too small)  \n",
    "- **oscillate** (jump around during training)  \n",
    "- **learn very slowly**  \n",
    "\n",
    "Normalization ensures that each layer receives inputs with a **consistent distribution**, which dramatically improves learning.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Why Normalization Is Needed\n",
    "\n",
    "### 1. **Internal Covariate Shift**\n",
    "\n",
    "Internal Covariate Shift refers to the phenomenon where the **distribution of activations inside the network keeps changing** as the model learns.\n",
    "\n",
    "**Why it happens:**\n",
    "\n",
    "- Layer 1 outputs activations `Œ±‚ÇÅ`\n",
    "- Layer 2 uses `Œ±‚ÇÅ` as input\n",
    "- When Layer 1 updates during training, the distribution of `Œ±‚ÇÅ` shifts\n",
    "- Layer 2 must constantly adapt to these new inputs\n",
    "\n",
    "This slows learning because **every layer keeps chasing a moving target**.\n",
    "\n",
    "Normalization (BatchNorm, LayerNorm, etc.) keeps the activations consistent, reducing this shift.\n",
    "\n",
    "**Effect:**  \n",
    "‚úÖ Faster convergence  \n",
    "‚úÖ More stable training  \n",
    "‚úÖ Higher accuracy  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Activation Drift**\n",
    "\n",
    "Activation drift means activations or gradients gradually start drifting toward:\n",
    "\n",
    "- **very large values** ‚Üí exploding gradients  \n",
    "- **very small values** ‚Üí vanishing gradients  \n",
    "\n",
    "Both issues break learning:\n",
    "\n",
    "- exploding gradients ‚Üí unstable updates  \n",
    "- vanishing gradients ‚Üí slow or zero learning  \n",
    "\n",
    "Normalization helps by keeping activations centered and scaled (e.g., mean ‚âà 0, variance ‚âà 1).\n",
    "\n",
    "**Effect:**  \n",
    "‚úÖ Gradients become stable  \n",
    "‚úÖ Training becomes smoother  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Distribution Drift (General Case)**\n",
    "\n",
    "Distribution Drift happens when the **statistical distribution of data** changes over time.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Input data distribution changes  \n",
    "- Training set and deployment data differ  \n",
    "- Internal activations drift as weights update  \n",
    "\n",
    "When distributions drift:\n",
    "\n",
    "- model predictions degrade  \n",
    "- training becomes inconsistent  \n",
    "- the model becomes harder to optimize  \n",
    "\n",
    "Normalization reduces internal drift and improves model robustness.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What Normalization Actually Does (Intuition)\n",
    "\n",
    "Normalization layers typically:\n",
    "\n",
    "1. Compute statistics (mean, variance, norms, etc.)  \n",
    "2. Use them to **scale and center** the activations  \n",
    "3. Optionally apply **learnable parameters**  \n",
    "   - gamma (Œ≥) ‚Üí scale  \n",
    "   - beta (Œ≤) ‚Üí shift  \n",
    "\n",
    "This ensures activations are:\n",
    "\n",
    "- not too large  \n",
    "- not too small  \n",
    "- well-conditioned for optimization  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Types of Normalization (Short Overview)\n",
    "\n",
    "### **1. Batch Normalization (BatchNorm)**\n",
    "- Normalizes across the batch dimension  \n",
    "- Works very well in CNNs  \n",
    "- Not ideal for small batch sizes or LLMs\n",
    "\n",
    "### **2. Layer Normalization (LayerNorm)**\n",
    "- Normalizes across features within a single token  \n",
    "- Used in Transformers and LLMs  \n",
    "- Works with batch size = 1  \n",
    "- No dependency on batch statistics\n",
    "\n",
    "### **3. RMSNorm**\n",
    "- Variant of LayerNorm without mean subtraction  \n",
    "- Common in newer LLMs (e.g., Falcon, LLaMA variants)\n",
    "\n",
    "### **4. GroupNorm / InstanceNorm**\n",
    "- Used mostly in computer vision architectures\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Why LLMs Prefer LayerNorm (instead of BatchNorm)\n",
    "\n",
    "- Batch sizes during inference = 1 ‚Üí BatchNorm fails  \n",
    "- Sequence lengths vary ‚Üí BatchNorm becomes inconsistent  \n",
    "- Transformers work token-wise ‚Üí LayerNorm fits naturally  \n",
    "\n",
    "LayerNorm keeps token representations stable, regardless of batch arrangement.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "Normalization solves several major problems:\n",
    "\n",
    "- ‚úÖ internal covariate shift  \n",
    "- ‚úÖ activation drift  \n",
    "- ‚úÖ distribution drift  \n",
    "- ‚úÖ exploding/vanishing gradients  \n",
    "- ‚úÖ slow/unstable training  \n",
    "\n",
    "By keeping activations stable, normalization enables deep networks ‚Äî especially Transformers and LLMs ‚Äî to train efficiently and reliably.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd1759",
   "metadata": {},
   "source": [
    "### ‚úÖ How Batch Normalization Works (Step-by-Step)\n",
    "\n",
    "For a given activation \\( x \\) during training:\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1 ‚Äî Compute Batch Mean**\n",
    "\n",
    "$$\n",
    "\\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2 ‚Äî Compute Batch Variance**\n",
    "\n",
    "$$\n",
    "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3 ‚Äî Normalize**\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "(Œµ avoids division by zero)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4c774",
   "metadata": {},
   "source": [
    "#### **Batch Normalization for Neural Nets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69759171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=128, out_features=30, bias=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "hidden_layer1 = torch.nn.Linear(128, 30)\n",
    "hidden_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7de4ec8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 30])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3119, -0.5689, -0.2118, -0.3213, -0.3049, -0.2963,  0.1679,  0.1045,\n",
       "         -0.4935, -0.3426,  0.1636,  0.3444,  0.4053, -0.3914, -0.0169, -0.4242,\n",
       "          0.2364, -0.0529,  0.1380,  0.2431, -0.1235, -0.1448,  0.4815, -0.0974,\n",
       "          0.2168, -0.1358,  0.1592,  0.0519,  0.2798,  0.0597],\n",
       "        [ 0.6261, -0.3463,  0.0785, -0.2130, -0.3671, -0.1707,  0.2661,  0.5566,\n",
       "         -0.5229, -0.1492, -0.0131,  0.4890,  0.2283, -0.0953, -0.2422, -0.0629,\n",
       "          0.3819, -0.3854, -0.0651,  0.0377, -0.0174,  0.2092,  0.6284, -0.0103,\n",
       "          0.4670,  0.3811,  0.0269,  0.1123,  0.3464,  0.2016]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size, hidden_size\n",
    "input_size = torch.rand(2, 128)\n",
    "\n",
    "outs = hidden_layer1(input_size)\n",
    "print(outs.shape)\n",
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec96b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll apply batch_normalization here \n",
    "# so for each batch we'll calculate mean and variance \n",
    "# after that all elements of that batch should be (x - u)/var^0.2 \n",
    "import torch \n",
    "\n",
    "def normalize(input):\n",
    "    # dim=-1 (as we want to calculate it for all elements of last dimension)\n",
    "    mean_without_keep_dim = torch.mean(outs, dim=-1)\n",
    "    mean_with_keep_dim = torch.mean(outs, dim=-1, keepdim=True)\n",
    "    print(f\"Mean without Keepdim : {mean_without_keep_dim.shape}\")\n",
    "    print(f\"Mean with Keepdim : {mean_with_keep_dim.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c44427c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean without Keepdim : torch.Size([2])\n",
      "Mean with Keepdim : torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "normalize(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b90fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making proper function for normalization\n",
    "def normalize(input):\n",
    "    print(f\"Input shape : {input.shape}\")\n",
    "    mean = torch.mean(outs, dim=-1)\n",
    "    var = torch.var(outs, dim=-1)\n",
    "    print(f\"Mean shape : {mean.shape}\")\n",
    "    inputs = (input - mean) / ((var)**(0.5) + 0.000001)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b16386ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape : torch.Size([2, 30])\n",
      "Mean shape : torch.Size([2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (30) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# this will throw an error as dimension of mean is not same as dimension of input\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# that's where we need keepdim=True parameter\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mouts\u001b[49m\u001b[43m)\u001b[49m.shape\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(input)\u001b[39m\n\u001b[32m      5\u001b[39m var = torch.var(outs, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMean shape : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m inputs = (\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m) / ((var)**(\u001b[32m0.5\u001b[39m) + \u001b[32m0.000001\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (30) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# this will throw an error as dimension of mean is not same as dimension of input\n",
    "# that's where we need keepdim=True parameter\n",
    "normalize(outs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d87df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization with keepdim=True\n",
    "def normalize(input):\n",
    "    print(f\"Input shape : {input.shape}\")\n",
    "    mean = torch.mean(outs, dim=-1, keepdim=True)\n",
    "    var = torch.var(outs, dim=-1, keepdim=True)\n",
    "    print(f\"Mean shape : {mean.shape}\")\n",
    "    inputs = (input - mean) / ((var)**(0.5) + 0.000001)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07eda9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape : torch.Size([2, 30])\n",
      "Mean shape : torch.Size([2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now this would do the normalization across batches\n",
    "normalize(outs).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611790f9",
   "metadata": {},
   "source": [
    "#### **Batch Normalization for CNNs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aedfc391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size : 2\n",
      "Channel size : 3\n",
      "Height of channel : 12\n",
      "Width of channel : 12\n"
     ]
    }
   ],
   "source": [
    "# CNNs comes with (batch_size, channels, height, width)\n",
    "cnn_output = torch.rand(2, 3, 12, 12)\n",
    "print(f\"Batch size : {cnn_output.shape[0]}\")\n",
    "print(f\"Channel size : {cnn_output.shape[1]}\")\n",
    "print(f\"Height of channel : {cnn_output.shape[2]}\")\n",
    "print(f\"Width of channel : {cnn_output.shape[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3fc360f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization in this case calculates mean and variance  for each channel of each batch\n",
    "\n",
    "def normalization_cnn(input):\n",
    "    # now we have a matrix (height x width) for which we have calculate mean and variance\n",
    "    mean = input.mean(dim=(0, 2, 3), keepdim=True)\n",
    "    # BatchNorm uses unbiased=False because it needs the true batch variance (divide by N), not the unbiased estimator (divide by N‚àí1).\n",
    "    var  = input.var(dim=(0, 2, 3), keepdim=True, unbiased=False)\n",
    "    print(mean.shape)\n",
    "\n",
    "    # 2) Normalize\n",
    "    input_hat = (input - mean) / torch.sqrt(var + 0.00001) \n",
    "    return input_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d8f5c868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 12, 12])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalization_cnn(cnn_output).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "21a22274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 12, 12])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for CNN this whole functionality is provided in BatchNorm2d(channels)\n",
    "\n",
    "batch_norm_layer = torch.nn.BatchNorm2d(3)\n",
    "output = batch_norm_layer(cnn_output)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "84fc1a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1, 1])\n",
      "Yes\n"
     ]
    }
   ],
   "source": [
    "## checking if my function and pytorch default BatchNorm Function works same or not\n",
    "if torch.allclose(output, normalization_cnn(cnn_output), atol=1e-5):\n",
    "    print(\"Yes\")\n",
    "else:\n",
    "    print(\"No\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28de1f31",
   "metadata": {},
   "source": [
    "#### **Layer Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432f60d",
   "metadata": {},
   "source": [
    "Let say we have a batch of 2 sentences with 5 words each and embed_dim = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f6c043e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, context_len, embed_dim\n",
    "inputs = torch.rand(2, 5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b6a710a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But here we don't use BatchNormalization, here we go with normalizing each token independently\n",
    "def layer_norm(input):\n",
    "    mean = torch.mean(input, dim=-1, keepdim=True)\n",
    "    var = torch.var(input, dim=-1, keepdim=True)\n",
    "    inputs = (input - mean) / torch.sqrt(var + 0.0001)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2dbd48af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 7])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = layer_norm(inputs)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee7295",
   "metadata": {},
   "source": [
    "#### **Layer Normalization Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a4d97a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.eps = 0.0001\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        var = torch.var(x, dim=-1, keepdim=True)\n",
    "        inputs = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0252095d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 7])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size, context_len, embed_dim\n",
    "inputs = torch.rand(2, 5, 7)\n",
    "layer_norm1 = LayerNormalization()\n",
    "outs = layer_norm1(inputs)\n",
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd35ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
