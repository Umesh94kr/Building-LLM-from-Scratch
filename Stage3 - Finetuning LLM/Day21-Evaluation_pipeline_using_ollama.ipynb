{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96df20dd",
   "metadata": {},
   "source": [
    "#### **Evaluation pipeline for Instruction Finetuned Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0412ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the data \n",
    "import json \n",
    "\n",
    "with open('instruction-data-with-response.json', 'r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3818640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in test data : 110\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of examples in test data : {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3fbb528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Rewrite the sentence using a simile.',\n",
       " 'input': 'The car is very fast.',\n",
       " 'output': 'The car is as fast as lightning.',\n",
       " 'model_response': 'The car is as fast as a cheetah.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## an example for test data \n",
    "test_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a87b08",
   "metadata": {},
   "source": [
    "##### **checking whether ollama in running or not**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d075c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1f895d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "def query_model(\n",
    "    prompt,\n",
    "    model=\"llama3\",\n",
    "    url=\"http://localhost:11434/api/chat\"\n",
    "):\n",
    "    # Create the data payload as a dictionary\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {     # Settings below are required for deterministic responses\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    # Convert the dictionary to a JSON formatted string and encode it to bytes\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    # Create a request object, setting the method to POST and adding necessary headers\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=payload,\n",
    "        method=\"POST\"\n",
    "    )\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # Send the request and capture the response\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        # Read and decode the response\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e184833f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in a llama's diet. They enjoy the sweet taste and texture of fresh hay.\n",
      "3. Grains: Llamas may receive grains like oats, barley, or corn as part of their daily ration. However, it's essential to provide these grains in moderation, as they can be high in calories.\n",
      "4. Fruits and vegetables: Llamas enjoy a variety of fruits and veggies, such as apples, carrots, sweet potatoes, and leafy greens like kale or spinach.\n",
      "5. Minerals: Llamas require access to mineral supplements, which help maintain their overall health and well-being.\n",
      "\n",
      "In the wild, llamas might also eat:\n",
      "\n",
      "1. Leaves: They'll munch on leaves from trees and shrubs, including plants like willow, alder, and birch.\n",
      "2. Bark: In some cases, llamas may eat the bark of certain trees, like aspen or cottonwood.\n",
      "3. Mosses and lichens: These non-vascular plants can be a tasty snack for llamas.\n",
      "\n",
      "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and fruits/vegetables. It's essential to consult with a veterinarian or experienced llama breeder to determine the best feeding plan for your llama.\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3\"\n",
    "result = query_model(\"What do Llamas eat?\", model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781b8521",
   "metadata": {},
   "source": [
    "#### **Prompt for Ollama**\n",
    "We'll write a prompt that instructs llama2 to compare actual response and model generated response, and score it in range of 0 to 100, later we'll save the outputs in a .txt file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "329eb77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(data):\n",
    "    return f\"\"\"\n",
    "    You are an evaluation agent. Your task is to compare the correctness and alignment of a model's response with respect to a given instruction and input.\n",
    "\n",
    "    You are provided with:\n",
    "    - Instruction: {data['instruction']}\n",
    "    - Input: {data['input']}\n",
    "    - Ground-truth Output: {data['output']}\n",
    "    - Finetuned Model Response: {data['model_response']}\n",
    "\n",
    "    Your goals:\n",
    "    1. Compare the finetuned model's response against the ground-truth output.\n",
    "    2. Evaluate how well the response satisfies the instruction.\n",
    "    3. Assign a similarity/quality score from 0 to 100.\n",
    "    4. Provide a brief explanation for the score.\n",
    "\n",
    "    **Important rules:**\n",
    "    - Base the score strictly on correctness, completeness, and faithfulness to the required instruction.\n",
    "    - Do NOT add extra fields.\n",
    "    - Do NOT alter the content of the instruction, input, output, or model response.\n",
    "\n",
    "    **Output format (return valid JSON only):**\n",
    "    {{\n",
    "        \"instruction\": \"{data['instruction']}\",\n",
    "        \"input\": \"{data['input']}\",\n",
    "        \"output\": \"{data['output']}\",\n",
    "        \"model_response\": \"{data['model_response']}\",\n",
    "        \"score\": <number between 0 and 100>,\n",
    "        \"reasoning\": \"<short explanation>\"\n",
    "    }}\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5617076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the evaluation result:\n",
      "\n",
      "{\n",
      "    \"instruction\": \"Rewrite the sentence using a simile.\",\n",
      "    \"input\": \"The car is very fast.\",\n",
      "    \"output\": \"The car is as fast as lightning.\",\n",
      "    \"model_response\": \"The car is as fast as a cheetah.\",\n",
      "    \"score\": 90,\n",
      "    \"reasoning\": \"The model's response is close to the ground-truth output, using a simile to compare the car's speed to that of an animal. While it doesn't exactly match the lightning example, it still effectively conveys the idea of the car being very fast.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# lets try with one example \n",
    "\n",
    "print(query_model(build_prompt(test_data[0]), model,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "560aaea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 10/110 [05:54<1:00:26, 36.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the evaluation result:\n",
      "\n",
      "{\n",
      "    \"instruction\": \"Classify the following numbers as prime or composite.\",\n",
      "    \"input\": \": 11, 14, 19.\",\n",
      "    \"output\": \"Prime numbers: 11, 19\n",
      "Composite numbers: 14\",\n",
      "    \"model_response\": \"Prime numbers: 11, 20\n",
      "Composite numbers: 20\",\n",
      "    \"score\": 66,\n",
      "    \"reasoning\": \"The model correctly identified the prime number 11 and incorrectly classified 20 as a prime number. It also correctly identified the composite number 14. However, it missed the other prime number 19 in its response.\"\n",
      "}\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 20/110 [11:55<57:41, 38.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the evaluation result:\n",
      "\n",
      "{\n",
      "    \"instruction\": \"Generate a sentence that follows the pattern: \\\"Never have I ever _____ without _____\\\"\",\n",
      "    \"input\": \"\",\n",
      "    \"output\": \"Never have I ever traveled without a map.\",\n",
      "    \"model_response\": \"The sentence \\\"Never have I ever\\\" contains an exclamation mark to show excitement.\",\n",
      "    \"score\": 20,\n",
      "    \"reasoning\": \"The model response does not follow the required pattern and does not generate a sentence that completes the phrase. The input is ignored, and the output is irrelevant to the task.\"\n",
      "}\n",
      "\n",
      "Score: 20\n",
      "\n",
      "Reasoning: The model response is not a correct completion of the given instruction. It provides an analysis of the original sentence instead of generating a new one that follows the pattern.\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 30/110 [17:46<44:23, 33.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the evaluation result:\n",
      "\n",
      "{\n",
      "    \"instruction\": \"Convert 5 miles to kilometers.\",\n",
      "    \"input\": \"\",\n",
      "    \"output\": \"5 miles is approximately 8.05 kilometers.\",\n",
      "    \"model_response\": \"5 miles is 5000 kilometers.\",\n",
      "    \"score\": 0,\n",
      "    \"reasoning\": \"The model response is completely incorrect, providing a conversion factor that is orders of magnitude off from the actual value. This indicates a lack of understanding of the instruction and the task at hand.\"\n",
      "}\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 40/110 [23:18<40:49, 34.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the evaluation result:\n",
      "\n",
      "{\n",
      "    \"instruction\": \"Explain what a sonnet is.\",\n",
      "    \"input\": \"\",\n",
      "    \"output\": \"A sonnet is a 14-line poem with a specific rhyme scheme and meter, often written in iambic pentameter.\",\n",
      "    \"model_response\": \"A sonnet is a set of defined, step-by-step procedures or rules to solve a problem or accomplish a task.\",\n",
      "    \"score\": 20,\n",
      "    \"reasoning\": \"The model's response does not accurately describe what a sonnet is. The correct definition provided in the ground-truth output highlights the poetic form and its characteristics, whereas the model's response incorrectly defines a sonnet as a set of procedures or rules.\"\n",
      "}\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 50/110 [29:19<37:46, 37.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the evaluation result:\n",
      "\n",
      "{\n",
      "    \"instruction\": \"Find a synonym for the given verb.\",\n",
      "    \"input\": \"Begin\",\n",
      "    \"output\": \"Commence\",\n",
      "    \"model_response\": \"A synonym for the word 'begin' could be 'start'.\",\n",
      "    \"score\": 60,\n",
      "    \"reasoning\": \"The model's response is partially correct, as 'start' can indeed be a synonym for 'begin'. However, it does not fully satisfy the instruction to find a synonym for the given verb. The ground-truth output 'Commence' is more accurate and directly answers the instruction.\"\n",
      "}\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 60/110 [36:31<34:47, 41.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the evaluation result:\n",
      "\n",
      "{\n",
      "    \"instruction\": \"Categorize the following sentence as either factual or opinion-based.\",\n",
      "    \"input\": \"Chocolate is the best dessert.\",\n",
      "    \"output\": \"Opinion-based.\",\n",
      "    \"model_response\": \"The topic of the given sentence is chocolate.\",\n",
      "    \"score\": 20,\n",
      "    \"reasoning\": \"The model's response does not directly answer the instruction, which specifically asks for categorization as factual or opinion-based. The provided response is also unrelated to the task and does not provide any insight into whether the sentence is factual or opinion-based.\"\n",
      "}\n",
      "\n",
      "Note: I assigned a score of 20 because the model's response is not relevant to the instruction and does not provide any correct information about the sentence being categorized.\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 70/110 [39:41<14:10, 21.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the evaluation result:\n",
      "\n",
      "{\n",
      "    \"instruction\": \"What is a synonym of 'sad'?\",\n",
      "    \"input\": \"\",\n",
      "    \"output\": \"A synonym for 'sad' is 'unhappy'.\",\n",
      "    \"model_response\": \"A synonym for 'sad' is 'ambitious'.\",\n",
      "    \"score\": 0,\n",
      "    \"reasoning\": \"The model's response does not provide a correct synonym of 'sad', as 'ambitious' is not a synonym of 'sad'. The ground-truth output correctly identifies 'unhappy' as a synonym, while the model's response is incorrect.\"\n",
      "}\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 80/110 [42:53<09:20, 18.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the evaluation result:\n",
      "\n",
      "{\n",
      "    \"instruction\": \"What is the abbreviation for 'Master of Business Administration'?\",\n",
      "    \"input\": \"\",\n",
      "    \"output\": \"The abbreviation for 'Master of Business Administration' is MBA.\",\n",
      "    \"model_response\": \"The abbreviation for 'Master of Business Administration' is 'MBA'.\",\n",
      "    \"score\": 100,\n",
      "    \"reasoning\": \"The model's response perfectly matches the ground-truth output, providing the correct answer to the given instruction.\"\n",
      "}\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 90/110 [46:28<07:05, 21.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the evaluation result:\n",
      "\n",
      "{\n",
      "    \"instruction\": \"What is the molecular formula for water?\",\n",
      "    \"input\": \"\",\n",
      "    \"output\": \"The molecular formula for water is H2O.\",\n",
      "    \"model_response\": \"The chemical formula for water (dihydrogen monoxide) is H2O.\",\n",
      "    \"score\": 100,\n",
      "    \"reasoning\": \"The model's response is identical to the ground-truth output, accurately providing the molecular formula for water as H2O. The response also correctly addresses the instruction and provides additional context in parentheses without affecting the correctness of the answer.\"\n",
      "}\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 100/110 [50:02<03:43, 22.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the evaluation result:\n",
      "\n",
      "{\n",
      "    \"instruction\": \"Generate a sentence using the word 'innovative'.\",\n",
      "    \"input\": \"\",\n",
      "    \"output\": \"The company's innovative approach set it apart from its competitors.\",\n",
      "    \"model_response\": \"Her innovative ideas led to the success of the project.\",\n",
      "    \"score\": 80,\n",
      "    \"reasoning\": \"The model response is mostly correct, but it does not exactly match the ground-truth output. The main difference is in the subject-verb agreement and sentence structure. The model's response uses 'her' as the subject, whereas the ground-truth output uses 'the company'. Additionally, the sentence structures are slightly different. However, both responses do use the word 'innovative' correctly.\"\n",
      "}\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [53:28<00:00, 29.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the evaluation result:\n",
      "\n",
      "{\n",
      "    \"instruction\": \"Convert 50 miles per hour to kilometers per hour.\",\n",
      "    \"input\": \"\",\n",
      "    \"output\": \"50 miles per hour is approximately 80.47 kilometers per hour.\",\n",
      "    \"model_response\": \"50 miles per hour is 0.5 kilometers per hour.\",\n",
      "    \"score\": 0,\n",
      "    \"reasoning\": \"The model response is completely incorrect, as 50 miles per hour cannot be converted to 0.5 kilometers per hour. The correct conversion is approximately 80.47 kilometers per hour.\"\n",
      "}\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Lets do it for whole script \n",
    "from tqdm import tqdm\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_no = 0\n",
    "    for data in tqdm(test_data):\n",
    "        response = query_model(build_prompt(data), model)\n",
    "        with open('evaluation_results.txt', 'a') as f:\n",
    "            f.write(response + \"\\n\\n\")\n",
    "        example_no += 1\n",
    "        if (example_no%10) == 0:\n",
    "            print(response)\n",
    "            print(f\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81310302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
