{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e13068ee",
   "metadata": {},
   "source": [
    "## Types of Tokenizers\n",
    "\n",
    "Tokenization is the process of breaking text into smaller units called **tokens** (words, subwords, or characters).  \n",
    "Different tokenization strategies affect vocabulary size, model understanding, and performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 1Ô∏è‚É£ **Word Tokenizer**\n",
    "\n",
    "**Definition:**  \n",
    "Splits text into individual **words** based on spaces or punctuation.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Input: \"I love lightning\" <br>\n",
    "Output: [\"I\", \"love\", \"lightning\"]\n",
    "\n",
    "**Drawback:**  \n",
    "Even similar words like `\"light\"` and `\"lightning\"` are treated as completely **different tokens**,  \n",
    "leading to:\n",
    "- A **larger vocabulary**\n",
    "- **No semantic link** between related words\n",
    "\n",
    "**üìâ Problem:**  \n",
    "Word tokenizers cannot generalize well to **unseen or rare words**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **Character Tokenizer**\n",
    "\n",
    "**Definition:**  \n",
    "Breaks text into **individual characters**, including spaces and punctuation.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Input: \"How are you\"<br>\n",
    "Output: [\"H\", \"o\", \"w\", \" \", \"a\", \"r\", \"e\", \" \", \"y\", \"o\", \"u\"]\n",
    "\n",
    "**Advantage:**  \n",
    "Very small vocabulary (only letters, digits, punctuation, etc.)\n",
    "\n",
    "**Drawback:**  \n",
    "Completely loses **semantic meaning** ‚Äî the model only sees letters, not words or their relationships.  \n",
    "\n",
    "It‚Äôs like teaching a model with *no concept of words.*\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ **Subword Tokenizer (Most Common)**\n",
    "\n",
    "**Definition:**  \n",
    "Breaks words into **meaningful smaller units (subwords)** ‚Äî like prefixes, suffixes, and roots.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Input: \"lightning\"<br>\n",
    "Output: [\"light\", \"##ning\"]<br>\n",
    "\n",
    "Input: \"unhappiness\"<br>\n",
    "Output: [\"un\", \"happi\", \"ness\"]\n",
    "\n",
    "**How it works:**  \n",
    "Uses algorithms like **Byte Pair Encoding (BPE)** or **WordPiece** to:\n",
    "- Merge frequent character pairs into subwords  \n",
    "- Keep common words as single tokens  \n",
    "- Split rare words into smaller known parts\n",
    "\n",
    "**Algorithm summary:**\n",
    "> ‚ÄúThe most frequent byte (or character) pairs should be combined into a single unit (token).‚Äù\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- Handles unseen words gracefully  \n",
    "- Keeps vocabulary size manageable  \n",
    "- Maintains meaning through reusable subword pieces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "912cf7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll use a library of python known as tiktoken, that implements BPE algorithm efficiently\n",
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cab03b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the tokenizer from gpt2\n",
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72adc885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10814, 1374, 389, 345, 36896, 28265, 1867, 389, 345, 1804, 826, 783, 11, 663, 43079, 2354, 13]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hey How are you Buddy!, What are you doing right now, its raining outside.\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "701c5d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey How are you Buddy!, What are you doing right now, its raining outside.\n"
     ]
    }
   ],
   "source": [
    "# converting these tokens back to words \n",
    "decoded_tokens = tokenizer.decode(tokens)\n",
    "print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94942af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
