{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e11ee8",
   "metadata": {},
   "source": [
    "### üß† Self-Attention  \n",
    "\n",
    "Embeddings of tokens are **independent of each other**, which means they **cannot capture the context** of a sentence.  \n",
    "\n",
    "**Self-Attention** helps overcome this by converting these **independent token embeddings** into **contextualized embeddings** ‚Äî where the representation of each word depends on all other words in the sentence.  \n",
    "\n",
    "In simple terms,  \n",
    "> The embedding of a word becomes a **weighted sum** of all other embeddings in the same sentence.\n",
    "\n",
    "---\n",
    "\n",
    "#### üì• Input  \n",
    "Input shape: (batch_size, context_len)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öôÔ∏è Problem with Simple Dot Product  \n",
    "If we only take the **dot product** of embeddings to compute attention scores, the same words across different sentences would get **identical scores**, even though their meanings may differ depending on context.\n",
    "\n",
    "---\n",
    "\n",
    "#### üí° Solution ‚Äî Introduce Query, Key, and Value  \n",
    "\n",
    "We introduce **three learnable matrices** that transform each embedding into different representations:\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "W_Q &: (\\text{context\\_len}, \\text{context\\_len}) \\\\\n",
    "W_K &: (\\text{context\\_len}, \\text{context\\_len}) \\\\\n",
    "W_V &: (\\text{context\\_len}, \\text{context\\_len})\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "These are used to generate:\n",
    "- **Q (Query):** What the current word is *looking for* in other words.  \n",
    "- **K (Key):** What each word *contains* that others might look for.  \n",
    "- **V (Value):** The actual information content of each word.  \n",
    "\n",
    "---\n",
    "\n",
    "#### üß© Self-Attention Formula  \n",
    "\n",
    "The **contextualized embedding** for each token is computed as:\n",
    "$$\n",
    "\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( QK^T \\) gives **similarity scores** between tokens  \n",
    "- \\( d_k \\) is the **dimension of the key vector**, used for scaling  \n",
    "- **softmax** normalizes the scores into probabilities  \n",
    "- Finally, multiplying by \\( V \\) gives a **weighted sum** of values (contextualized embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a292ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context size : (6), Embedding dim : (3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2428, 0.9521, 0.8071],\n",
       "        [0.4264, 0.6224, 0.9260],\n",
       "        [0.9122, 0.2526, 0.2183],\n",
       "        [0.4540, 0.9181, 0.6634],\n",
       "        [0.4926, 0.6716, 0.5819],\n",
       "        [0.8921, 0.5329, 0.0452]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# this is one sentence input\n",
    "inputs = torch.rand(6 , 3)\n",
    "print(f\"Context size : ({inputs.shape[0]}), Embedding dim : ({inputs.shape[1]})\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ac3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for making query, key, value \n",
    "din = inputs.shape[1]\n",
    "dout = 3\n",
    "\n",
    "Wq = torch.nn.Parameter(torch.rand(din, dout), requires_grad=False)\n",
    "Wk = torch.nn.Parameter(torch.rand(din, dout), requires_grad=False)\n",
    "Wv = torch.nn.Parameter(torch.rand(din, dout), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffea0a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries shape : torch.Size([6, 3])\n",
      "Keys shape : torch.Size([6, 3])\n",
      "Values shape : torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "queries = inputs @ Wq.weight\n",
    "keys = inputs @ Wk.weight\n",
    "values = inputs @ Wv.weight\n",
    "\n",
    "print(f\"Queries shape : {queries.shape}\")\n",
    "print(f\"Keys shape : {keys.shape}\")\n",
    "print(f\"Values shape : {values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d46d734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of attention scores : torch.Size([6, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8795e+00, -2.8734e+00, -1.0750e+00, -1.1436e+00, -2.5227e+00,\n",
       "         -1.1112e-01],\n",
       "        [-2.3830e+00, -3.7004e+00, -1.3775e+00, -1.4084e+00, -3.2607e+00,\n",
       "         -3.9009e-02],\n",
       "        [-8.9538e-01, -1.3941e+00, -4.2938e-01, -4.8184e-01, -1.1525e+00,\n",
       "         -4.5575e-04],\n",
       "        [-1.4712e+00, -2.2130e+00, -7.8770e-01, -8.9895e-01, -1.8970e+00,\n",
       "         -1.4746e-01],\n",
       "        [-1.9428e+00, -3.0416e+00, -1.0526e+00, -1.0919e+00, -2.6192e+00,\n",
       "          1.8658e-02],\n",
       "        [-9.7863e-01, -1.3951e+00, -5.2670e-01, -6.6487e-01, -1.1975e+00,\n",
       "         -2.3683e-01]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we need to calculate attention score by getting dot product\n",
    "# For now it should be 6 x 6 matrix as we have 6 words so each word will have 6 weights \n",
    "\n",
    "attention_scores = queries @ keys.T\n",
    "print(f\"Shape of attention scores : {attention_scores.shape}\")\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28427fd5",
   "metadata": {},
   "source": [
    "### ‚öñÔ∏è Why Do We Divide by ‚àöd in Self-Attention?\n",
    "\n",
    "In the **self-attention mechanism**, we compute similarity between tokens using the dot product of **Query (Q)** and **Key (K)** vectors:\n",
    "\n",
    "$$\n",
    "\\text{scores} = QK^T\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üí° The Problem: Large Dot Products\n",
    "\n",
    "When the embedding dimension \\( d_k \\) is large, each element in \\( Q \\) and \\( K \\) contributes to the sum ‚Äî making the **dot product values grow very large**.\n",
    "\n",
    "These large values are passed through a **softmax** function:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "$$\n",
    "\n",
    "If \\( x_i \\) are too large:\n",
    "- \\( e^{x_i} \\) becomes extremely large  \n",
    "- Softmax output becomes **very peaky** (one value ‚âà 1, others ‚âà 0)  \n",
    "- Gradients become **tiny** ‚Üí leading to **unstable training**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ The Solution: Scaling by \\( \\sqrt{d_k} \\)\n",
    "\n",
    "To fix this, we **scale down** the dot product by dividing it by the **square root of the key dimension**:\n",
    "\n",
    "$$\n",
    "\\text{Scaled Scores} = \\frac{QK^T}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "This ensures that:\n",
    "- The variance of the dot product remains roughly **constant (‚âà 1)**  \n",
    "- Softmax values stay in a **reasonable range**  \n",
    "- Training becomes **stable and smooth**\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Intuitive Analogy\n",
    "\n",
    "Think of \\( \\sqrt{d_k} \\) as a **normalization factor**.\n",
    "\n",
    "When vectors have more dimensions (larger \\( d_k \\)), their dot products naturally become larger ‚Äî like louder ‚Äúsignals‚Äù.  \n",
    "Dividing by \\( \\sqrt{d_k} \\) **keeps the volume consistent**, so the model doesn‚Äôt get overwhelmed.\n",
    "\n",
    "---\n",
    "\n",
    "### üî¢ Example (Conceptually)\n",
    "\n",
    "If elements of \\( Q \\) and \\( K \\) each have mean 0 and variance 1:\n",
    "\n",
    "$$\n",
    "\\text{Var}(QK^T) = d_k\n",
    "$$\n",
    "\n",
    "Then dividing by \\( \\sqrt{d_k} \\) gives:\n",
    "\n",
    "$$\n",
    "\\text{Var}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) = 1\n",
    "$$\n",
    "\n",
    "‚úÖ This keeps the scores well-scaled across different embedding dimensions, ensuring **consistent attention behavior**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e02dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores_new = attention_scores / torch.sqrt(torch.tensor(din, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91a6ba97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1227, 0.0691, 0.1952, 0.1877, 0.0846, 0.3406],\n",
       "        [0.1055, 0.0493, 0.1884, 0.1851, 0.0635, 0.4081],\n",
       "        [0.1456, 0.1092, 0.1906, 0.1849, 0.1255, 0.2441],\n",
       "        [0.1341, 0.0874, 0.1990, 0.1866, 0.1049, 0.2880],\n",
       "        [0.1161, 0.0615, 0.1940, 0.1897, 0.0785, 0.3602],\n",
       "        [0.1493, 0.1174, 0.1938, 0.1789, 0.1316, 0.2291]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we need to apply softmax on the scores to make them stable and interpretable \n",
    "attention_scores_sclaed = torch.softmax(attention_scores_new, dim=-1)\n",
    "attention_scores_sclaed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c643ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of context inputs : torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "# Now we need to get the contextualized embeddings for each input word in sentence\n",
    "context_inputs = attention_scores_sclaed @ values\n",
    "print(f\"Shape of context inputs : {context_inputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea141ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1820, -0.1630, -1.4938],\n",
      "        [ 0.1591, -0.0893, -1.5360],\n",
      "        [ 0.2222, -0.3157, -1.4101],\n",
      "        [ 0.2032, -0.2354, -1.4522],\n",
      "        [ 0.1751, -0.1382, -1.5092],\n",
      "        [ 0.2294, -0.3440, -1.3900]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(context_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff0bf1a",
   "metadata": {},
   "source": [
    "See how different contextualized word embeddings are as compared to input embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda2fba2",
   "metadata": {},
   "source": [
    "#### Self-Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63b370b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.Wq = torch.nn.Parameter(torch.rand(din, dout))\n",
    "        self.Wk = torch.nn.Parameter(torch.rand(din, dout))\n",
    "        self.Wv = torch.nn.Parameter(torch.rand(din, dout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is input senetence (context_len, embedding_dim)\n",
    "        # calculate key, query and value\n",
    "        keys = x @ self.Wk\n",
    "        queries = x @ self.Wq \n",
    "        values = x @ self.Wv \n",
    "\n",
    "        # attention scores \n",
    "        attn_scores = queries @ keys.T \n",
    "        # scaling attention scores \n",
    "        attn_scores = attn_scores / torch.sqrt(torch.tensor(x.shape[1], dtype=torch.float32))\n",
    "\n",
    "        scaled_attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        contextualized_inputs = scaled_attn_weights @ values \n",
    "        return contextualized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be70ac90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7492, 0.9378, 0.9723],\n",
       "        [0.0987, 0.7818, 0.0461],\n",
       "        [0.0569, 0.5981, 0.5543],\n",
       "        [0.1916, 0.9791, 0.7765],\n",
       "        [0.4310, 0.1584, 0.6319],\n",
       "        [0.6062, 0.5110, 0.2282]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attention = SelfAttention(3, 3)\n",
    "inputs2 = torch.rand(6, 3)\n",
    "inputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87d91d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4659, 1.3326, 1.3747],\n",
       "        [0.4154, 1.1671, 1.2156],\n",
       "        [0.4203, 1.1841, 1.2316],\n",
       "        [0.4483, 1.2763, 1.3203],\n",
       "        [0.4114, 1.1536, 1.2027],\n",
       "        [0.4207, 1.1842, 1.2328]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextualized_inputs2 = self_attention(inputs2)\n",
    "contextualized_inputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020d0575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
